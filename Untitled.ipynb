{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    PROIECT SEMESTRU IC1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by GIRLA Ionut 411-SIVA\n",
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"E:/SIVA-Master/AN_1/Sem_1/IC1/proiect_semestru/dataset/jaffedbase\"\n",
    "CATEGORIES = []\n",
    "\n",
    "train_ratio = 0.85\n",
    "test_ratio = 0.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KA.AN1.39.tiff\n",
      "KA-AN1\n",
      "# HAP SAD SUR ANG DIS PIC\n",
      "\n",
      "['HAP', 'SAD', 'SUR', 'ANG', 'DIS']\n"
     ]
    }
   ],
   "source": [
    "labels = open(\"labels.txt\")\n",
    "for img in os.listdir(DATADIR):\n",
    "    print(img)\n",
    "    new_string = img.split(sep=\".\")[0] + \"-\" + img.split(sep=\".\")[1]\n",
    "    print(new_string)\n",
    "    break\n",
    "    \n",
    "with open(\"labels.txt\") as f:\n",
    "    first_line = f.readline()\n",
    "    print(first_line)\n",
    "    CATEGORIES = first_line.split(sep=\" \")[1:-1]\n",
    "    print(CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(213, 2)\n",
      "(256, 256)\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "def import_dataset():\n",
    "    contor = 0\n",
    "    for img in os.listdir(DATADIR):\n",
    "            try:\n",
    "                #print(f\"suntem la pasul {contor}\")\n",
    "                new_string = img.split(sep=\".\")[0] + \"-\" + img.split(sep=\".\")[1]\n",
    "                img_array = cv2.imread(os.path.join(DATADIR,img), cv2.IMREAD_GRAYSCALE)\n",
    "                labels = open(\"labels.txt\")\n",
    "                for line in labels:\n",
    "                    #print(f\"line = {line}\")\n",
    "                    if new_string in line:\n",
    "                        #print(f\"new_string = {new_string}; with line = {line}\")\n",
    "                        result = line.split(sep=\" \")[1:-1] \n",
    "                        max_val = result[0]\n",
    "                        index = 0\n",
    "                        #print(len(result))\n",
    "                        for i in range (1,len(result)):\n",
    "                            #print(f\"i = {i} ; result[i] = {result[i]}; max_val = {max_val}; index = {index}\")\n",
    "                            if max_val < result[i]:\n",
    "                                index = i\n",
    "                                max_val = result[i]\n",
    "                #print(index)\n",
    "                labels.close()\n",
    "                dataset.append([img_array, index])\n",
    "                contor = contor + 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "dataset = [] \n",
    "import_dataset()\n",
    "print(np.shape(dataset))\n",
    "print(np.shape(dataset[0][0]))\n",
    "print(np.shape(dataset[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " size of x_train: 181, size of x_test: 32\n",
      " size of x_train: (181, 65536), size of x_test: (32, 65536)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for features, label in dataset:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "    \n",
    "#X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)  #to be uncommented if chosen to resize imgs\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio)\n",
    "\n",
    "x_train = np.reshape(x_train,(len(x_train),256*256))\n",
    "x_test = np.reshape(x_test,((len(X)-len(x_train)),256*256))\n",
    "print(f\" size of x_train: {len(x_train)}, size of x_test: {len(x_test)}\")\n",
    "\n",
    "print(f\" size of x_train: {np.shape(x_train)}, size of x_test: {np.shape(x_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  0  0  1  2]\n",
      " [ 0  1  0  0  2]\n",
      " [ 0  0  4  1  0]\n",
      " [ 0  0  0  3  0]\n",
      " [ 2  0  2  0 10]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "#clf.fit(data_train, y_train)\n",
    "#y_predicted_svm = clf.predict(data_test)\n",
    "clf = svm.SVC()\n",
    "clf.fit(x_train,y_train)\n",
    "y_predicted_svm = clf.predict(x_test)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true = y_test[:] ,y_pred = y_predicted_svm)\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precizie = 0.6875\n"
     ]
    }
   ],
   "source": [
    "contor = 0\n",
    "for i in range (len(y_test)):\n",
    "    if (y_test[i] == y_predicted_svm[i]):\n",
    "        contor = contor + 1\n",
    "\n",
    "print(f\"precizie = {contor/len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 50)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19856796 0.16973472 0.11620265 0.05917904 0.04966852 0.03614874\n",
      " 0.03428682 0.0253514  0.02109139 0.01727601 0.01532298 0.01391863\n",
      " 0.01116031 0.00999423 0.00887713 0.00849618 0.00783388 0.00756239\n",
      " 0.00667501 0.00621374 0.00610358 0.00570274 0.00519727 0.00470582\n",
      " 0.00454115 0.00427301 0.00413475 0.00381042 0.00376724 0.00349271\n",
      " 0.00342281 0.0032364  0.00300087 0.00279922 0.00273859 0.00265772\n",
      " 0.00261764 0.00245401 0.00240161 0.00236423 0.00228232 0.00213838\n",
      " 0.00208866 0.00201455 0.00193692 0.00186387 0.00184387 0.00180286\n",
      " 0.0017368  0.00165939]\n"
     ]
    }
   ],
   "source": [
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  1  0  1  1]\n",
      " [ 0  2  0  0  1]\n",
      " [ 0  0  4  1  0]\n",
      " [ 0  0  0  3  0]\n",
      " [ 2  0  1  0 11]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "#clf.fit(data_train, y_train)\n",
    "#y_predicted_svm = clf.predict(data_test)\n",
    "clf_after_pca = svm.SVC()\n",
    "clf_after_pca.fit(x_train,y_train)\n",
    "y_predicted_svm = clf_after_pca.predict(x_test)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true = y_test[:] ,y_pred = y_predicted_svm)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precizie = 0.75\n"
     ]
    }
   ],
   "source": [
    "contor = 0\n",
    "for i in range (len(y_test)):\n",
    "    if (y_test[i] == y_predicted_svm[i]):\n",
    "        contor = contor + 1\n",
    "\n",
    "print(f\"precizie = {contor/len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "x_train = sc_X.fit_transform(x_train)\n",
    "x_test = sc_X.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
